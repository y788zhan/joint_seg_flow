{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import json\n",
    "import socket\n",
    "from PIL import Image\n",
    "from util import *\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hyperParams.json\") as f:\n",
    "    instanceParams = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "instanceParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logPath = \"logs/\"\n",
    "snapshotPath = \"snapshots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printFrequency = instanceParams[\"printFreq\"]\n",
    "snapshotFrequency = instanceParams[\"snapFreq\"]\n",
    "batchSize = instanceParams[\"batchSize\"]\n",
    "\n",
    "iterations = instanceParams[\"iterations\"]\n",
    "baseLearningRate = instanceParams[\"baseLR\"]\n",
    "learningRate = baseLearningRate\n",
    "snapshotFrequency = instanceParams[\"snapshotFreq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume, startIteration, snapshotFiles = checkResume(snapshotPath,logPath, {'resume': 'n'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used to generate train image txt files from DAVIS's train.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_list = None\n",
    "# with open('../example_data/data/DAVIS/ImageSets/480p/train.txt', 'r') as f:\n",
    "#     img_list = f.readlines()\n",
    "# img_list = [x.strip() for x in img_list]\n",
    "# img_list = [item.split(' ')[0] for item in img_list]\n",
    "# img_list = [\"data/DAVIS\" + item for item in img_list]\n",
    "# img_list\n",
    "\n",
    "# objects = sorted(['bear','car-shadow','elephant','lucia','rollerblade','blackswan','car-turn','flamingo','mallard-fly','scooter-black',\n",
    "# 'bmx-bumps','cows','goat','mallard-water','scooter-gray','bmx-trees','dance-jump','hike','motocross-bumps','soapbox','boat','dance-twirl',\n",
    "# 'hockey','motocross-jump','soccerball','breakdance','dog','horsejump-high','motorbike','stroller','breakdance-flare',\n",
    "# 'dog-agility','horsejump-low','paragliding','surf','bus','drift-chicane','kite-surf','paragliding-launch','swing','camel',\n",
    "# 'drift-straight','kite-walk','parkour','tennis','car-roundabout','drift-turn','libby','rhino','train'])\n",
    "\n",
    "# objects = ['/' + item + '/' for item in objects]\n",
    "# count = {}\n",
    "# for item in objects:\n",
    "#     count[item] = 0\n",
    "\n",
    "# for im in img_list:\n",
    "#     for item in objects:\n",
    "#         if item in im:\n",
    "#             count[item] += 1\n",
    "\n",
    "# for key in objects:\n",
    "#     try:\n",
    "#         if count[key] == 0:\n",
    "#             del count[key]\n",
    "#     except:\n",
    "#         print key\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# with open('../example_data/datalists/train_im0.txt', 'w') as f:\n",
    "#     for key in sorted(count.keys()):\n",
    "#         l = count[key]\n",
    "#         for j in range(l-1):\n",
    "#             f.write(\"%s\\n\" % img_list[i])\n",
    "#             i += 1\n",
    "#         i += 1\n",
    "\n",
    "# i = 0\n",
    "# with open('../example_data/datalists/train_im1.txt', 'w') as f:\n",
    "#     for key in sorted(count.keys()):\n",
    "#         l = count[key]\n",
    "#         i += 1\n",
    "#         for j in range(l-1):\n",
    "#             f.write(\"%s\\n\" % img_list[i])\n",
    "#             i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    trainingData = TrainingData(batchSize,instanceParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    # init\n",
    "    with tf.variable_scope(\"netShare\"):\n",
    "        networkBodyF = NetworkBody(trainingData,instanceParams)\n",
    "    with tf.variable_scope(\"netShare\",reuse=True):\n",
    "        networkBodyB = NetworkBody(trainingData,instanceParams,flipInput=True)\n",
    "\n",
    "    trainingLoss = TrainingLoss(instanceParams,networkBodyF,networkBodyB,trainingData)\n",
    "    solver,learningRateTensor = attachSolver(trainingLoss.loss)\n",
    "\n",
    "    # loss scheduling\n",
    "    recLossBWeightTensor = trainingLoss.recLossBWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver(max_to_keep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "arg = DotMap()\n",
    "arg.logDev = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printFrequency = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start\n",
    "with sessionSetup(arg) as sess:\n",
    "#     if resume:\n",
    "#         saver.restore(sess,snapshotPath+snapshotFiles[-1][:-6])\n",
    "#     else:\n",
    "#         sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess,\n",
    "                  '../model_download_scripts/photometric_smoothness/weights/iter_0000000000500000.ckpt')\n",
    "\n",
    "    trainingData.dataQueuer.start_queueing(sess)\n",
    "\n",
    "    #start summary writer\n",
    "    summary_writer = tf.summary.FileWriter(logPath, sess.graph)\n",
    "\n",
    "    #run\n",
    "    lastPrint = time.time()\n",
    "    for i in range(500000, 500000 + iterations):\n",
    "        # scheduled values\n",
    "        learningRate = learningRateSchedule(baseLearningRate, i)\n",
    "        recLossBWeight = unsupLossBSchedule(i)\n",
    "\n",
    "         #run training\n",
    "        feed_dict = {\n",
    "            learningRateTensor: learningRate,\n",
    "            recLossBWeightTensor: recLossBWeight,\n",
    "        }\n",
    "        summary,result,totalLoss = sess.run([merged,solver,trainingLoss.loss], feed_dict=feed_dict)\n",
    "\n",
    "        if (i+1) % printFrequency == 0:\n",
    "            timeDiff = time.time() - lastPrint\n",
    "            itPerSec = printFrequency/timeDiff\n",
    "            remainingIt = iterations - i\n",
    "            eta = remainingIt/itPerSec\n",
    "            print(\"Iteration \"+str(i+1)+\": loss: \"+str(totalLoss)+\", iterations per second: \"+str(itPerSec)+\", ETA: \"+str(datetime.timedelta(seconds=eta)))+\", lr: \"+str(learningRate)\n",
    "\n",
    "            summary_writer.add_summary(summary,i+1)\n",
    "            summary_writer.flush()\n",
    "            lastPrint = time.time()\n",
    "\n",
    "        if (i+1) % snapshotFrequency == 0:\n",
    "            saver.save(sess,\"snapshots/iter_\"+str(i+1).zfill(16)+\".ckpt\")\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    #close queing\n",
    "    trainingData.dataQueuer.close(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "This is just a hack to view a random estimated flow ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start\n",
    "with sessionSetup(arg) as sess:\n",
    "    saver.restore(sess,\n",
    "                  '../model_download_scripts/photometric_smoothness/weights/iter_0000000000500000.ckpt')\n",
    "\n",
    "    trainingData.dataQueuer.start_queueing(sess)\n",
    "\n",
    "    #start summary writer\n",
    "    summary_writer = tf.summary.FileWriter(logPath, sess.graph)\n",
    "\n",
    "    #run\n",
    "    flowFinal = networkBodyF.flows[0]\n",
    "    flowViz = flowToRgb(flowFinal)\n",
    "    for i in range(500000, 500000 + 1):\n",
    "        # scheduled values\n",
    "        learningRate = learningRateSchedule(baseLearningRate, i)\n",
    "        recLossBWeight = unsupLossBSchedule(i)\n",
    "\n",
    "         #run training\n",
    "        feed_dict = {\n",
    "            learningRateTensor: learningRate,\n",
    "            recLossBWeightTensor: recLossBWeight,\n",
    "        }\n",
    "        flow,summary,result,totalLoss = sess.run([flowViz,merged,solver,trainingLoss.loss], feed_dict=feed_dict)\n",
    "\n",
    "    # close queing\n",
    "    trainingData.dataQueuer.close(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.minimum(np.asarray(flow),1)\n",
    "arr = np.maximum(arr,0)\n",
    "arr = np.squeeze(np.asarray(arr*255,np.uint8))\n",
    "im = Image.fromarray(arr[0])\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
