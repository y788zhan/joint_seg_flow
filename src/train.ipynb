{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import json\n",
    "import socket\n",
    "from PIL import Image\n",
    "from util import *\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "logPath = \"training/\"\n",
    "snapshotPath = \"snapshots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'baseLR': 1e-05,\n",
       " u'batchSize': 2,\n",
       " u'borderThicknessH': 0.01,\n",
       " u'borderThicknessW': 0.01,\n",
       " u'boundaryAlpha': 6.5,\n",
       " u'dataset': u'kitti2012',\n",
       " u'flowScale': 20.0,\n",
       " u'gradParams': {u'robustness': 0.46, u'scale': 255.0, u'weight': 6.408},\n",
       " u'instanceName': u'unsupFlownet',\n",
       " u'iterations': 500000,\n",
       " u'lossComponents': {u'asymmetricSmooth': True,\n",
       "  u'backward': False,\n",
       "  u'boundaries': False,\n",
       "  u'gradient': False,\n",
       "  u'smooth2nd': False},\n",
       " u'photoParams': {u'robustness': 0.53, u'scale': 360.0},\n",
       " u'printFreq': 100,\n",
       " u'resnet': False,\n",
       " u'smooth2ndParams': {u'robustness': 0.21, u'scale': 1.0, u'weight': 0.53},\n",
       " u'smoothOccParams': {u'robustness': 0.9, u'scale': 1.8},\n",
       " u'smoothParams': {u'robustness': 0.28, u'scale': 3.5, u'weight': 0.64},\n",
       " u'snapFreq': 5000,\n",
       " u'snapshotFreq': 10000,\n",
       " u'weightDecay': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"hyperParams.json\") as f:\n",
    "    instanceParams = json.load(f)\n",
    "\n",
    "instanceParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No snapshots found, training from scratch\n"
     ]
    }
   ],
   "source": [
    "printFrequency = instanceParams[\"printFreq\"]\n",
    "snapshotFrequency = instanceParams[\"snapFreq\"]\n",
    "batchSize = instanceParams[\"batchSize\"]\n",
    "\n",
    "iterations = instanceParams[\"iterations\"]\n",
    "baseLearningRate = instanceParams[\"baseLR\"]\n",
    "learningRate = baseLearningRate\n",
    "snapshotFrequency = instanceParams[\"snapshotFreq\"]\n",
    "\n",
    "\n",
    "\n",
    "from dotmap import DotMap\n",
    "arg = DotMap()\n",
    "arg.logDev = False\n",
    "arg.resume = 'y'\n",
    "resume, startIteration, snapshotFiles = checkResume(snapshotPath,logPath, arg)\n",
    "iterations = 1000 * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code used to generate train image txt files from DAVIS's train.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Tensor(\"ImagePairData/image_data_reader/Placeholder:0\", dtype=string, device=/device:GPU:0)\n",
      "Reading Tensor(\"ImagePairData/image_data_reader_1/Placeholder:0\", dtype=string, device=/device:GPU:0)\n",
      "Reading Tensor(\"ImagePairData/image_data_reader_2/Placeholder:0\", dtype=string, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From components/rgbToGray.py:10: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From components/convLayer.py:8: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    trainingData = TrainingData(batchSize,instanceParams)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    # init\n",
    "    with tf.variable_scope(\"netShare\"):\n",
    "        networkBodyF = NetworkBody(trainingData,instanceParams)\n",
    "    with tf.variable_scope(\"netShare\",reuse=True):\n",
    "        networkBodyB = NetworkBody(trainingData,instanceParams,flipInput=True)\n",
    "\n",
    "    trainingLoss = TrainingLoss(instanceParams,networkBodyF,networkBodyB,trainingData)\n",
    "    solver,learningRateTensor = attachSolver(trainingLoss.loss)\n",
    "\n",
    "    # loss scheduling\n",
    "    recLossBWeightTensor = trainingLoss.recLossBWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summaries\n",
    "merged = tf.summary.merge_all()\n",
    "# saver\n",
    "saver = tf.train.Saver(max_to_keep=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model_download_scripts/photometric_smoothness/weights/iter_0000000000500000.ckpt\n",
      "Iteration 500100: loss: [[49.266235]\n",
      " [30.332296]], iterations per second: 1.16948768904, ETA: -5 days, 3:35:28.465863, lr: 1.25e-06\n",
      "Iteration 500200: loss: [[47.7024  ]\n",
      " [38.763184]], iterations per second: 1.43089391132, ETA: -4 days, 0:50:17.649120, lr: 1.25e-06\n",
      "Iteration 500300: loss: [[34.976307]\n",
      " [63.656944]], iterations per second: 1.43221282845, ETA: -4 days, 0:54:23.309348, lr: 1.25e-06\n",
      "Iteration 500400: loss: [[77.419106]\n",
      " [84.77728 ]], iterations per second: 1.44917843395, ETA: -4 days, 2:00:02.057751, lr: 1.25e-06\n",
      "Iteration 500500: loss: [[28.02604 ]\n",
      " [10.730354]], iterations per second: 1.43647055846, ETA: -4 days, 1:08:58.772610, lr: 1.25e-06\n",
      "Iteration 500600: loss: [[52.564774]\n",
      " [30.86668 ]], iterations per second: 1.438229011, ETA: -4 days, 1:14:46.730661, lr: 1.25e-06\n",
      "Iteration 500700: loss: [[34.29627 ]\n",
      " [23.786419]], iterations per second: 1.45109179564, ETA: -4 days, 2:04:01.517213, lr: 1.25e-06\n",
      "Iteration 500800: loss: [[88.69957]\n",
      " [20.23027]], iterations per second: 1.45277307265, ETA: -4 days, 2:09:24.030129, lr: 1.25e-06\n"
     ]
    }
   ],
   "source": [
    "# start\n",
    "with sessionSetup(arg) as sess:\n",
    "#     if resume:\n",
    "#         saver.restore(sess,snapshotPath+snapshotFiles[-1][:-6])\n",
    "#     else:\n",
    "#         sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess,\n",
    "                  '../model_download_scripts/photometric_smoothness/weights/iter_0000000000500000.ckpt')\n",
    "\n",
    "    trainingData.dataQueuer.start_queueing(sess)\n",
    "\n",
    "    #start summary writer\n",
    "    summary_writer = tf.summary.FileWriter(logPath, sess.graph)\n",
    "\n",
    "    #run\n",
    "    lastPrint = time.time()\n",
    "    for i in range(500000, 500000 + iterations + 10):\n",
    "        # scheduled values\n",
    "        learningRate = learningRateSchedule(baseLearningRate, i)\n",
    "        recLossBWeight = unsupLossBSchedule(i)\n",
    "\n",
    "         #run training\n",
    "        feed_dict = {\n",
    "            learningRateTensor: learningRate,\n",
    "            recLossBWeightTensor: recLossBWeight,\n",
    "        }\n",
    "        summary,result,totalLoss = sess.run([merged,solver,trainingLoss.loss], feed_dict=feed_dict)\n",
    "\n",
    "        if (i+1) % printFrequency == 0:\n",
    "            timeDiff = time.time() - lastPrint\n",
    "            itPerSec = printFrequency/timeDiff\n",
    "            remainingIt = iterations - i\n",
    "            eta = remainingIt/itPerSec\n",
    "            print(\"Iteration \"+str(i+1)+\": loss: \"+str(totalLoss)+\", iterations per second: \"+str(itPerSec)+\", ETA: \"+str(datetime.timedelta(seconds=eta)))+\", lr: \"+str(learningRate)\n",
    "\n",
    "            summary_writer.add_summary(summary,i+1)\n",
    "            summary_writer.flush()\n",
    "            lastPrint = time.time()\n",
    "\n",
    "        if (i+1) % snapshotFrequency == 0:\n",
    "            saver.save(sess,\"snapshots/iter_\"+str(i+1).zfill(16)+\".ckpt\")\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    #close queing\n",
    "    trainingData.dataQueuer.close(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRoot = '../example_data/'\n",
    "frame0Path = '../example_data/datalists/train_im0.txt'\n",
    "frame1Path = '../example_data/datalists/train_im1.txt'\n",
    "desiredHeight = 480\n",
    "desiredWidth = 854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(frame0Path) as f:\n",
    "    imagePairs0 = [datasetRoot+x[:-1] for x in f.readlines()]\n",
    "with open(frame1Path) as f:\n",
    "    imagePairs1 = [datasetRoot+x[:-1] for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = TestData(imagePairs0,imagePairs1,1,desiredHeight,desiredWidth)\n",
    "#393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    with tf.variable_scope(\"netShare\"):\n",
    "        networkBody = NetworkBody(testData,instanceParams)\n",
    "flowFinal = networkBody.flows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowViz = flowToRgb(flowFinal)\n",
    "transformGrid = flowTransformGrid(flowFinal)\n",
    "mean = [0.407871, 0.457525, 0.481094]\n",
    "mean = tf.expand_dims(tf.expand_dims(tf.expand_dims(mean,0),0),0)\n",
    "# warped = flowWarp(testData.frame1[\"rgb\"]+mean,flowFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# config tensorflow\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "iterations = 510000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 70\n",
    "result_dir = 'results'\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess,\"snapshots/iter_\"+str(iterations).zfill(16)+\".ckpt\")\n",
    "\n",
    "    # run\n",
    "    lastPrint = time.time()\n",
    "    for i in range(697, 697 + test_length):\n",
    "        feed_dict = {\n",
    "            testData.im0File: imagePairs0[i],\n",
    "            testData.im1File: imagePairs1[i],\n",
    "        }\n",
    "        result = sess.run([flowViz,testData.height,testData.width],feed_dict=feed_dict)\n",
    "        h = result[1]\n",
    "        w = result[2]\n",
    "\n",
    "        arr = np.maximum(np.minimum(np.asarray(result[0]),1), 0)\n",
    "        arr = np.squeeze(np.asarray(arr*255,np.uint8))\n",
    "        im = Image.fromarray(arr[:h,:w,:])\n",
    "        im.save(\"{}/{}.png\".format(result_dir, str(i).zfill(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
