{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import json\n",
    "import socket\n",
    "from PIL import Image\n",
    "from util import *\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib notebook\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "logPath = \"TRAINING/split/train/\"\n",
    "snapshotPath = \"SNAPSHOTS/snapshots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"hyperParams.json\") as f:\n",
    "    instanceParams = json.load(f)\n",
    "\n",
    "# instanceParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No snapshots found, training from scratch\n"
     ]
    }
   ],
   "source": [
    "printFrequency = instanceParams[\"printFreq\"]\n",
    "batchSize = instanceParams[\"batchSize\"]\n",
    "\n",
    "iterations = instanceParams[\"iterations\"]\n",
    "baseLearningRate = instanceParams[\"baseLR\"]\n",
    "learningRate = baseLearningRate\n",
    "snapshotFrequency = instanceParams[\"snapshotFreq\"]\n",
    "\n",
    "from dotmap import DotMap\n",
    "arg = DotMap()\n",
    "arg.logDev = False\n",
    "arg.resume = 'n'\n",
    "resume, startIteration, snapshotFiles = checkResume(snapshotPath,logPath, arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Tensor(\"ImagePairData/image_data_reader/Placeholder:0\", dtype=string, device=/device:GPU:0)\n",
      "Reading Tensor(\"ImagePairData/image_data_reader_1/Placeholder:0\", dtype=string, device=/device:GPU:0)\n",
      "Reading Tensor(\"ImagePairData/image_data_reader_2/Placeholder:0\", dtype=string, device=/device:GPU:0)\n",
      "Reading Tensor(\"ImagePairData/image_data_reader_3/Placeholder:0\", dtype=string, device=/device:GPU:0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From components/rgbToGray.py:10: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    trainingData = TrainingData(batchSize,instanceParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    flow_gd = tf.Variable(tf.zeros([2, 448, 832, 2]), name = \"my_new_var\")\n",
    "    photoAlpha = instanceParams[\"photoParams\"][\"robustness\"]\n",
    "    photoBeta = instanceParams[\"photoParams\"][\"scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From components/convLayer.py:8: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    # init\n",
    "    with tf.variable_scope(\"netShare\"):\n",
    "        networkBodyF = NetworkBody(trainingData,instanceParams)\n",
    "    with tf.variable_scope(\"netShare\",reuse=True):\n",
    "        networkBodyB = NetworkBody(trainingData,instanceParams,flipInput=True)\n",
    "\n",
    "    trainingLoss = TrainingLoss(instanceParams,networkBodyF,networkBodyB,trainingData, flow_gd)\n",
    "    solver, solver_op, learningRateTensor = attachSolver(trainingLoss.loss)\n",
    "    flowFinal = networkBodyF.flows[0]\n",
    "    # loss scheduling\n",
    "    recLossBWeightTensor = trainingLoss.recLossBWeight\n",
    "    golTensor = trainingLoss.gol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    gamma = trainingLoss.gol * instanceParams[\"smoothParams\"][\"weight\"]\n",
    "    sd_lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    ploss = photoLoss(flow_gd,\n",
    "                      trainingData.frame0[\"rgbNorm\"],\n",
    "                      trainingData.frame1[\"rgbNorm\"],\n",
    "                      photoAlpha, photoBeta)\n",
    "    closs = tf.square(flow_gd - tf.stop_gradient(flowFinal))\n",
    "    closs = tf.reduce_sum(closs * gamma, axis=3, keep_dims=True)\n",
    "    tf.summary.image(\"fhat\", flowToRgb1(flow_gd))\n",
    "    tf.summary.scalar(\"fhat_ploss\", tf.reduce_mean(ploss))\n",
    "    tf.summary.scalar(\"gamma\", tf.reduce_mean(gamma))\n",
    "\n",
    "    with tf.variable_scope(None, default_name=\"new_var_solver\"):\n",
    "        split_solver = tf.train.AdamOptimizer(learning_rate=sd_lr, beta1=0.9, beta2=0.999)\n",
    "        split_solver_op = split_solver.minimize(ploss + closs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "var_subset = []\n",
    "uninit = []\n",
    "for v in variables:\n",
    "    if not (\"new_var\" in v.name):\n",
    "        var_subset.append(v)\n",
    "    else:\n",
    "        uninit.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summaries\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver(var_subset, max_to_keep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "2000\n",
      "60000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "printFrequency = 100\n",
    "snapshotFrequency = 2000\n",
    "iterations = 60000\n",
    "startIteration = 0\n",
    "print(printFrequency)\n",
    "print(snapshotFrequency)\n",
    "print(iterations)\n",
    "print(startIteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# golSchedule = 10 * (1 - np.exp(-np.arange(0, 20000) / 5000.0)) # np.flip(10 - np.logspace(-4, 1, 1e4))\n",
    "# plt.plot(golSchedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golSchedule(prev=None, flow=None, fhat=None, lr=None):\n",
    "    ret = prev + lr * np.square(flow - fhat)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/y788zhang/.conda/envs/p27/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "fs = []\n",
    "fhats = []\n",
    "with sessionSetup(arg) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # sess.run(tf.initialize_variables(uninit))\n",
    "    # saver.restore(sess, '../model_download_scripts/photometric_smoothness/weights/iter_0000000000500000.ckpt')\n",
    "    trainingData.dataQueuer.start_queueing(sess)\n",
    "\n",
    "    #start summary writer\n",
    "    summary_writer = tf.summary.FileWriter(logPath, sess.graph)\n",
    "\n",
    "    #run\n",
    "    lastPrint = time.time()\n",
    "    for i in range(startIteration, startIteration + iterations + 10):\n",
    "        # scheduled values\n",
    "        learningRate = learningRateSchedule(baseLearningRate, i)\n",
    "        recLossBWeight = unsupLossBSchedule(i)\n",
    "\n",
    "         #run training\n",
    "        feed_dict = {\n",
    "            learningRateTensor: 1e-5,\n",
    "            golTensor: np.ones((2, 448, 832, 2)) * 10.0 # if i == 0 else golSchedule(gol, f1, f2, 1e-3)\n",
    "        }\n",
    "        summary,result,totalLoss, f1, nfg, pl, f2, gol = sess.run([\n",
    "            merged,solver_op,trainingLoss.loss, flowFinal, split_solver_op,\n",
    "            ploss, flow_gd, golTensor], feed_dict=feed_dict)\n",
    "\n",
    "        if (i+1) % printFrequency == 0:\n",
    "            timeDiff = time.time() - lastPrint\n",
    "            itPerSec = printFrequency/timeDiff\n",
    "            remainingIt = startIteration + iterations + 10 - i\n",
    "            eta = remainingIt/itPerSec\n",
    "            print(\"Iteration \"+str(i+1)+\": loss: \"+str(totalLoss)+\", iterations per second: \"+str(\n",
    "                itPerSec)+\", ETA: \"+str(datetime.timedelta(seconds=eta)))+\", lr: \"+str(learningRate)\n",
    "\n",
    "            summary_writer.add_summary(summary,i+1)\n",
    "            summary_writer.flush()\n",
    "            lastPrint = time.time()\n",
    "            fs.append(deepcopy(f1))\n",
    "            fhats.append(deepcopy(f2))\n",
    "\n",
    "        if (i+1) % snapshotFrequency == 0:\n",
    "            saver.save(sess,\"snapshots/iter_\"+str(i+1).zfill(16)+\".ckpt\")\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    trainingData.dataQueuer.close(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10396073"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(fs[-1] - fhats[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRoot = '../example_data/'\n",
    "frame0Path = '../example_data/datalists/train_im0.txt'\n",
    "frame1Path = '../example_data/datalists/train_im1.txt'\n",
    "gt0Path = '../example_data/datalists/train_gt0.txt'\n",
    "desiredHeight = 480\n",
    "desiredWidth = 854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(frame0Path) as f:\n",
    "    imagePairs0 = [datasetRoot+x[:-1] for x in f.readlines()]\n",
    "with open(frame1Path) as f:\n",
    "    imagePairs1 = [datasetRoot+x[:-1] for x in f.readlines()]\n",
    "with open(gt0Path) as f:\n",
    "    gtPairs1 = [datasetRoot+x[:-1] for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePairs0 = [item for item in imagePairs0 if 'bus' in item]\n",
    "imagePairs1 = [item for item in imagePairs1 if 'bus' in item]\n",
    "gtPairs1 = [item for item in gtPairs1 if 'bus' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = TestData(imagePairs0,imagePairs1,gtPairs1,1,desiredHeight,desiredWidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    with tf.variable_scope(\"netShare\"):\n",
    "        networkBody = NetworkBody(testData,instanceParams)\n",
    "    flowFinal = networkBody.flows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowViz = flowToRgb(flowFinal)\n",
    "transformGrid = flowTransformGrid(flowFinal)\n",
    "mean = tf.expand_dims(tf.expand_dims(tf.expand_dims([0.407871, 0.457525, 0.481094], 0), 0), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# config tensorflow\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = 70\n",
    "start = 0\n",
    "result_dir = 'results'\n",
    "iterations = 515000\n",
    "\n",
    "flows = []\n",
    "viz = []\n",
    "gradients = []\n",
    "ground_truths = []\n",
    "arrs = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess,\"snapshots/iter_0000000000002000.ckpt\")\n",
    "\n",
    "    # run\n",
    "    lastPrint = time.time()\n",
    "    for i in range(start, start + min(test_length, len(imagePairs0))):\n",
    "        feed_dict = {\n",
    "            testData.im0File: imagePairs0[i],\n",
    "            testData.im1File: imagePairs1[i],\n",
    "            testData.gt0File: gtPairs1[i]\n",
    "        }\n",
    "        hsv, f = sess.run([flowViz, flowFinal],feed_dict=feed_dict)\n",
    "\n",
    "        flows.append(deepcopy(f))\n",
    "        h, w = 448, 532\n",
    "        arr = np.maximum(np.minimum(np.asarray(hsv),1), 0)\n",
    "        arr = np.squeeze(np.asarray(arr*255,np.uint8))\n",
    "        arrs.append(deepcopy(arr[:h,:w,:]))\n",
    "        im = Image.fromarray(arr[:h,:w,:])\n",
    "        viz.append(deepcopy(im))\n",
    "        # im.save(\"{}/{}.png\".format(result_dir, str(i).zfill(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
